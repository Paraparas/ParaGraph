{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio to Transcript conversion\n",
    "\n",
    "This code allows you to convert from raw audio of a meeting to a time-annotated and speaker diarized transcript.\n",
    "\n",
    "Import this code into Google Colab to use with T4."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "8e8-HHJm05Bj",
    "outputId": "f448c4d1-bf66-4108-99bf-0f06da161374"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting git+https://github.com/openai/whisper.git\n",
      "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-and6gq9b\n",
      "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-and6gq9b\n",
      "  Resolved https://github.com/openai/whisper.git to commit 90db0de1896c23cbfaf0c58bc2d30665f709f170\n",
      "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.60.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (1.26.4)\n",
      "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (2.5.1+cu121)\n",
      "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (4.66.6)\n",
      "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (10.5.0)\n",
      "Requirement already satisfied: tiktoken in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (0.8.0)\n",
      "Requirement already satisfied: triton>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20240930) (3.1.0)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton>=2.0.0->openai-whisper==20240930) (3.16.1)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20240930) (0.43.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2024.9.11)\n",
      "Requirement already satisfied: requests>=2.26.0 in /usr/local/lib/python3.10/dist-packages (from tiktoken->openai-whisper==20240930) (2.32.3)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (4.12.2)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (2024.10.0)\n",
      "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20240930) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->openai-whisper==20240930) (1.3.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.26.0->tiktoken->openai-whisper==20240930) (2024.8.30)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->openai-whisper==20240930) (3.0.2)\n",
      "Requirement already satisfied: pyannote.audio in /usr/local/lib/python3.10/dist-packages (3.3.2)\n",
      "Requirement already satisfied: asteroid-filterbanks>=0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.4.0)\n",
      "Requirement already satisfied: einops>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.8.0)\n",
      "Requirement already satisfied: huggingface-hub>=0.13.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.26.3)\n",
      "Requirement already satisfied: lightning>=2.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.4.0)\n",
      "Requirement already satisfied: omegaconf<3.0,>=2.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.3.0)\n",
      "Requirement already satisfied: pyannote.core>=5.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (5.0.0)\n",
      "Requirement already satisfied: pyannote.database>=5.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (5.1.0)\n",
      "Requirement already satisfied: pyannote.metrics>=3.2 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (3.2.1)\n",
      "Requirement already satisfied: pyannote.pipeline>=3.0.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (3.0.1)\n",
      "Requirement already satisfied: pytorch-metric-learning>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.7.0)\n",
      "Requirement already satisfied: rich>=12.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (13.9.4)\n",
      "Requirement already satisfied: semver>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (3.0.2)\n",
      "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.12.1)\n",
      "Requirement already satisfied: speechbrain>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (1.0.2)\n",
      "Requirement already satisfied: tensorboardX>=2.6 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.6.2.2)\n",
      "Requirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.5.1+cu121)\n",
      "Requirement already satisfied: torch-audiomentations>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (0.11.1)\n",
      "Requirement already satisfied: torchaudio>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (2.5.1+cu121)\n",
      "Requirement already satisfied: torchmetrics>=0.11.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.audio) (1.6.0)\n",
      "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (1.26.4)\n",
      "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from asteroid-filterbanks>=0.4->pyannote.audio) (4.12.2)\n",
      "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (3.16.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2024.10.0)\n",
      "Requirement already satisfied: packaging>=20.9 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (6.0.2)\n",
      "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.13.0->pyannote.audio) (4.66.6)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio) (0.11.9)\n",
      "Requirement already satisfied: pytorch-lightning in /usr/local/lib/python3.10/dist-packages (from lightning>=2.0.1->pyannote.audio) (2.4.0)\n",
      "Requirement already satisfied: antlr4-python3-runtime==4.9.* in /usr/local/lib/python3.10/dist-packages (from omegaconf<3.0,>=2.1->pyannote.audio) (4.9.3)\n",
      "Requirement already satisfied: sortedcontainers>=2.0.4 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (2.4.0)\n",
      "Requirement already satisfied: scipy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.core>=5.0.0->pyannote.audio) (1.13.1)\n",
      "Requirement already satisfied: pandas>=0.19 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (2.2.2)\n",
      "Requirement already satisfied: typer>=0.12.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.database>=5.0.1->pyannote.audio) (0.15.0)\n",
      "Requirement already satisfied: scikit-learn>=0.17.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.5.2)\n",
      "Requirement already satisfied: docopt>=0.6.2 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.6.2)\n",
      "Requirement already satisfied: tabulate>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (0.9.0)\n",
      "Requirement already satisfied: matplotlib>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (3.8.0)\n",
      "Requirement already satisfied: sympy>=1.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.metrics>=3.2->pyannote.audio) (1.13.1)\n",
      "Requirement already satisfied: optuna>=3.1 in /usr/local/lib/python3.10/dist-packages (from pyannote.pipeline>=3.0.1->pyannote.audio) (4.1.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=12.0.0->pyannote.audio) (2.18.0)\n",
      "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.10/dist-packages (from soundfile>=0.12.1->pyannote.audio) (1.17.1)\n",
      "Requirement already satisfied: hyperpyyaml in /usr/local/lib/python3.10/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (1.2.2)\n",
      "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (1.4.2)\n",
      "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.10/dist-packages (from speechbrain>=1.0.0->pyannote.audio) (0.2.0)\n",
      "Requirement already satisfied: protobuf>=3.20 in /usr/local/lib/python3.10/dist-packages (from tensorboardX>=2.6->pyannote.audio) (4.25.5)\n",
      "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->pyannote.audio) (3.1.4)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy>=1.1->pyannote.metrics>=3.2->pyannote.audio) (1.3.0)\n",
      "Requirement already satisfied: julius<0.3,>=0.2.3 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (0.2.7)\n",
      "Requirement already satisfied: librosa>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (0.10.2.post1)\n",
      "Requirement already satisfied: torch-pitch-shift>=1.2.2 in /usr/local/lib/python3.10/dist-packages (from torch-audiomentations>=0.11.0->pyannote.audio) (1.2.5)\n",
      "Requirement already satisfied: pycparser in /usr/local/lib/python3.10/dist-packages (from cffi>=1.0->soundfile>=0.12.1->pyannote.audio) (2.22)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (3.11.9)\n",
      "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (3.0.1)\n",
      "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (4.4.2)\n",
      "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.60.0)\n",
      "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (1.8.2)\n",
      "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.5.0.post1)\n",
      "Requirement already satisfied: lazy-loader>=0.1 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.4)\n",
      "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.10/dist-packages (from librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (1.1.0)\n",
      "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from lightning-utilities<2.0,>=0.10.0->lightning>=2.0.1->pyannote.audio) (75.1.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=12.0.0->pyannote.audio) (0.1.2)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.3.1)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (4.55.1)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.4.7)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (11.0.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (3.2.0)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (2.8.2)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.14.0)\n",
      "Requirement already satisfied: colorlog in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.10/dist-packages (from optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (2.0.36)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas>=0.19->pyannote.database>=5.0.1->pyannote.audio) (2024.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn>=0.17.1->pyannote.metrics>=3.2->pyannote.audio) (3.5.0)\n",
      "Requirement already satisfied: primePy>=1.3 in /usr/local/lib/python3.10/dist-packages (from torch-pitch-shift>=1.2.2->torch-audiomentations>=0.11.0->pyannote.audio) (1.3)\n",
      "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (8.1.7)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.1->pyannote.database>=5.0.1->pyannote.audio) (1.5.4)\n",
      "Requirement already satisfied: ruamel.yaml>=0.17.28 in /usr/local/lib/python3.10/dist-packages (from hyperpyyaml->speechbrain>=1.0.0->pyannote.audio) (0.18.6)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->pyannote.audio) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.4.0)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2.2.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->huggingface-hub>=0.13.0->pyannote.audio) (2024.8.30)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.3.1)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (4.0.3)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (24.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (0.2.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning>=2.0.1->pyannote.audio) (1.18.3)\n",
      "Requirement already satisfied: Mako in /usr/local/lib/python3.10/dist-packages (from alembic>=1.5.0->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (1.3.8)\n",
      "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba>=0.51.0->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (0.43.0)\n",
      "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.10/dist-packages (from pooch>=1.1->librosa>=0.6.0->torch-audiomentations>=0.11.0->pyannote.audio) (4.3.6)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib>=2.0.0->pyannote.metrics>=3.2->pyannote.audio) (1.16.0)\n",
      "Requirement already satisfied: ruamel.yaml.clib>=0.2.7 in /usr/local/lib/python3.10/dist-packages (from ruamel.yaml>=0.17.28->hyperpyyaml->speechbrain>=1.0.0->pyannote.audio) (0.2.12)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from sqlalchemy>=1.4.2->optuna>=3.1->pyannote.pipeline>=3.0.1->pyannote.audio) (3.1.1)\n",
      "Reading package lists... Done\n",
      "Building dependency tree... Done\n",
      "Reading state information... Done\n",
      "ffmpeg is already the newest version (7:4.4.2-0ubuntu0.22.04.1).\n",
      "0 upgraded, 0 newly installed, 0 to remove and 49 not upgraded.\n"
     ]
    }
   ],
   "source": [
    "!pip install git+https://github.com/openai/whisper.git\n",
    "!pip install pyannote.audio\n",
    "!apt-get install -y ffmpeg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZiMJ4VDf2y22"
   },
   "outputs": [],
   "source": [
    "from google.colab import userdata\n",
    "HF_TOKEN = userdata.get('HF_TOKEN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n4LfXzOw1Iml",
    "outputId": "ff691721-ad90-4f54-ba9f-9f613d783502"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing diarization...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:pytorch_lightning.utilities.migration.utils:Lightning automatically upgraded your loaded checkpoint from v1.5.4 to v2.4.0. To apply the upgrade to your files permanently, run `python -m pytorch_lightning.utilities.upgrade_checkpoint ../root/.cache/torch/pyannote/models--pyannote--segmentation/snapshots/c4c8ceafcbb3a7a280c2d357aee9fbc9b0be7f9b/pytorch_model.bin`\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model was trained with pyannote.audio 0.0.1, yours is 3.3.2. Bad things might happen unless you revert pyannote.audio to 0.x.\n",
      "Model was trained with torch 1.10.0+cu102, yours is 2.5.1+cu121. Bad things might happen unless you revert torch to 1.x.\n",
      "...Diarization complete.\n",
      "Performing transcription...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 139M/139M [00:01<00:00, 76.6MiB/s]\n",
      "/usr/local/lib/python3.10/dist-packages/whisper/__init__.py:150: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint = torch.load(fp, map_location=device)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "... Transcription complete.\n",
      "Merging results...\n",
      "... Merging results complete.\n",
      "[\n",
      "  {\n",
      "    \"start\": 0.0,\n",
      "    \"end\": 4.44,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"So actually getting back to a kin as like a question\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 4.44,\n",
      "    \"end\": 5.12,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"at the beginning.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 5.12,\n",
      "    \"end\": 10.36,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"So for now, probably we just need to make a really quick\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 10.36,\n",
      "    \"end\": 15.200000000000001,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"decision probably within today, like if we want to have\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 15.200000000000001,\n",
      "    \"end\": 17.400000000000002,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"the ASR module, right?\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 17.400000000000002,\n",
      "    \"end\": 22.400000000000002,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"So if we want to have the ASR module,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 22.400000000000002,\n",
      "    \"end\": 25.76,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"we can allow users to upload recordings.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 25.76,\n",
      "    \"end\": 29.080000000000002,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"But now actually it's optional because like users,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 29.08,\n",
      "    \"end\": 31.279999999999998,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"we can start from their transcripts.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 31.279999999999998,\n",
      "    \"end\": 35.04,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"So it would be better if we have that ASR module,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 35.04,\n",
      "    \"end\": 38.72,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"but if it means that implementation,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 38.72,\n",
      "    \"end\": 41.519999999999996,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"if it would take another probably a few days,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 41.519999999999996,\n",
      "    \"end\": 44.32,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"that's like just too much because we want to focus\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 44.32,\n",
      "    \"end\": 48.599999999999994,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"on integration with like open AI or cloud\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 48.599999999999994,\n",
      "    \"end\": 51.64,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"and a better visualization.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 51.64,\n",
      "    \"end\": 55.599999999999994,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"And then we can also mention we have the capability\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 55.6,\n",
      "    \"end\": 59.6,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"because we are some ASR developers, researchers,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 59.6,\n",
      "    \"end\": 62.36,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"we can always integrate with the ASR later.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 62.36,\n",
      "    \"end\": 65.72,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"It's not that mid and then actually that's our plan.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 65.72,\n",
      "    \"end\": 70.72,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"So we really need like you decide for us,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 72.48,\n",
      "    \"end\": 76.04,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"like you think it's manageable.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 76.04,\n",
      "    \"end\": 77.04,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"So let's do it.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 77.04,\n",
      "    \"end\": 80.68,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"So you think it's too much, we can maybe do it later.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 80.68,\n",
      "    \"end\": 81.84,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"Yeah.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 82.8,\n",
      "    \"end\": 90.80000000000001,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"OK, so I think that the ASR integration is definitely possible.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 91.80000000000001,\n",
      "    \"end\": 95.60000000000001,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"One thing that might be harder to do is,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 95.60000000000001,\n",
      "    \"end\": 97.52000000000001,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"because like for instance, we could definitely\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 97.52000000000001,\n",
      "    \"end\": 102.16,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"pass the entire audio of the meeting\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 102.16,\n",
      "    \"end\": 105.4,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"into some like a model like Whisper or something.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 105.4,\n",
      "    \"end\": 107.72,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"But I think one thing, something that might be harder\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 107.72,\n",
      "    \"end\": 111.04,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"or things like like a speaker diarization.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 111.04,\n",
      "    \"end\": 116.4,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"So basically indicating who is speaking when.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 116.4,\n",
      "    \"end\": 119.52000000000001,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"And that's something that is maybe a little bit harder\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 119.52000000000001,\n",
      "    \"end\": 124.04,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"if you're just going from like a raw audio recording\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 124.04,\n",
      "    \"end\": 126.56,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"of the whole meeting.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 126.56,\n",
      "    \"end\": 129.96,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"Because I think like with Google Meet with their API,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 129.96,\n",
      "    \"end\": 134.44,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"I think that they're actually recording each person's audio\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 134.44,\n",
      "    \"end\": 137.04000000000002,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"individually and so they can, yeah, exactly.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 137.04,\n",
      "    \"end\": 141.32,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"Yeah, and so they can more easily,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 141.32,\n",
      "    \"end\": 143.64,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"always seems like how and left.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 143.64,\n",
      "    \"end\": 148.0,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"But they can more easily identify who is speaking at each time.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 152.35999999999999,\n",
      "    \"end\": 152.88,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"Yeah.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 152.88,\n",
      "    \"end\": 156.48,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"Yeah, so basically if we use Google Meet,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 156.48,\n",
      "    \"end\": 158.84,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"not only the speaker diarization,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 158.84,\n",
      "    \"end\": 161.35999999999999,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"we even got the speaker identification\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 161.35999999999999,\n",
      "    \"end\": 163.04,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"like for free instantly.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 163.04,\n",
      "    \"end\": 165.48,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"So yeah, also maybe later.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 165.48,\n",
      "    \"end\": 168.64,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"So while we make it into products, we want, yeah,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 168.64,\n",
      "    \"end\": 172.12,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"make sure we integrate with all those like a major\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 172.12,\n",
      "    \"end\": 176.48,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"like a meeting application to Google Meet, Microsoft,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 176.48,\n",
      "    \"end\": 181.72,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"like it means it will be a feature in our plan.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 181.72,\n",
      "    \"end\": 185.95999999999998,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"But probably actually we really need you to work more\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 185.95999999999998,\n",
      "    \"end\": 188.2,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"on the integration with large language model.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 188.2,\n",
      "    \"end\": 191.51999999999998,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"Because actually in my mind, I still have a few more.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 191.51999999999998,\n",
      "    \"end\": 193.92,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"I mean more advanced feature.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 194.04,\n",
      "    \"end\": 196.64,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"So probably I don't know if we have time,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 196.64,\n",
      "    \"end\": 201.04,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"but definitely we want to like focus on something more,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 201.04,\n",
      "    \"end\": 205.88,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"I mean, and just like focus for the demo.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 205.88,\n",
      "    \"end\": 206.83999999999997,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"Yes.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 206.83999999999997,\n",
      "    \"end\": 207.67999999999998,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"Hi, Mahama.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 207.67999999999998,\n",
      "    \"end\": 210.51999999999998,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"Can you hear us?\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 210.51999999999998,\n",
      "    \"end\": 211.27999999999997,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"Yeah.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 211.27999999999997,\n",
      "    \"end\": 214.6,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"So I'm sorry, the disturbance you didn't have the question.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 214.6,\n",
      "    \"end\": 215.6,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"Yeah.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 215.6,\n",
      "    \"end\": 217.32,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"I know where is it because we have the recording.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 217.32,\n",
      "    \"end\": 218.07999999999998,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"Yeah.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 218.07999999999998,\n",
      "    \"end\": 219.6,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"OK, no worries.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 219.6,\n",
      "    \"end\": 220.6,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"OK.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 220.6,\n",
      "    \"end\": 221.11999999999998,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"OK.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 221.11999999999998,\n",
      "    \"end\": 222.27999999999997,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"OK.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 222.28,\n",
      "    \"end\": 225.08,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"Well, so I guess I guess maybe to summarize.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 225.08,\n",
      "    \"end\": 230.76,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"So I think that I can maybe I'll try to look into\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 230.76,\n",
      "    \"end\": 234.08,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"or what tools or things might exist right now\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 234.08,\n",
      "    \"end\": 241.8,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"to go from like a raw recording to a transcript.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 241.8,\n",
      "    \"end\": 242.76,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"I think it's possible.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 242.76,\n",
      "    \"end\": 245.0,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"And it probably also doesn't have to be perfect.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 245.0,\n",
      "    \"end\": 249.48,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"So like for instance, maybe even without the speaker\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 249.48,\n",
      "    \"end\": 256.4,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"identification, the model could somehow make something\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 256.4,\n",
      "    \"end\": 259.12,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"that would be acceptable for the demo,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 259.12,\n",
      "    \"end\": 264.2,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"or it might be able to deduce the speaker from the context.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 264.2,\n",
      "    \"end\": 267.12,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"But yeah.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 267.12,\n",
      "    \"end\": 271.0,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"So I think I'll look into it and then I'll let you guys know.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 271.0,\n",
      "    \"end\": 273.15999999999997,\n",
      "    \"speaker\": \"SPEAKER_00\",\n",
      "    \"text\": \"Yeah.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 273.15999999999997,\n",
      "    \"end\": 277.36,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"Yeah, so basically, I mean, because our entire pipeline\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 277.36,\n",
      "    \"end\": 280.84000000000003,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"is from either the recording or the transcript\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 280.84000000000003,\n",
      "    \"end\": 285.84000000000003,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"to the large length of model and to the parse, the label,\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 285.84000000000003,\n",
      "    \"end\": 289.16,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"the data, and finally to our visualization layer.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 289.16,\n",
      "    \"end\": 291.88,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"So as long as we have those, we can\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 291.88,\n",
      "    \"end\": 297.2,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"ignore the quality of those like process data\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 297.2,\n",
      "    \"end\": 299.84000000000003,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"like after the large length of model.\"\n",
      "  },\n",
      "  {\n",
      "    \"start\": 299.84000000000003,\n",
      "    \"end\": 305.16,\n",
      "    \"speaker\": \"SPEAKER_01\",\n",
      "    \"text\": \"So actually, we don't care the source.\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "import subprocess\n",
    "from pyannote.audio import Pipeline\n",
    "import whisper\n",
    "import tempfile\n",
    "import torch\n",
    "\n",
    "# -----------------------------\n",
    "# Configuration\n",
    "# -----------------------------\n",
    "INPUT_AUDIO_FILE = \"meeting_audio_5min.wav\"  # Update with your input file name/path\n",
    "DIARIZATION_MODEL = \"pyannote/speaker-diarization\"  # Pretrained diarization model\n",
    "WHISPER_MODEL = \"base\"  # \"tiny\", \"base\", \"small\", \"medium\", \"large\", etc.\n",
    "\n",
    "# -----------------------------\n",
    "# Preprocessing (Optional)\n",
    "# If your file is not in the correct format (mono, 16kHz), you can uncomment:\n",
    "# -----------------------------\n",
    "# def preprocess_audio(input_file, output_file):\n",
    "#     # Convert to 16kHz, mono wav\n",
    "#     cmd = [\n",
    "#         \"ffmpeg\",\n",
    "#         \"-y\",\n",
    "#         \"-i\", input_file,\n",
    "#         \"-ar\", \"16000\",\n",
    "#         \"-ac\", \"1\",\n",
    "#         output_file\n",
    "#     ]\n",
    "#     subprocess.run(cmd, check=True)\n",
    "#\n",
    "# # Example usage:\n",
    "# preprocessed_file = \"processed_audio.wav\"\n",
    "# preprocess_audio(INPUT_AUDIO_FILE, preprocessed_file)\n",
    "# INPUT_AUDIO_FILE = preprocessed_file\n",
    "\n",
    "# -----------------------------\n",
    "# Speaker Diarization\n",
    "# -----------------------------\n",
    "def perform_diarization(audio_file):\n",
    "    pipeline = Pipeline.from_pretrained(DIARIZATION_MODEL,\n",
    "                                        use_auth_token=HF_TOKEN)\n",
    "    pipeline.to(torch.device('cuda'))\n",
    "    diarization = pipeline(audio_file)\n",
    "\n",
    "    # Extract segments: (start, end, speaker_label)\n",
    "    diarization_segments = []\n",
    "    for segment, _, speaker in diarization.itertracks(yield_label=True):\n",
    "        diarization_segments.append((segment.start, segment.end, speaker))\n",
    "\n",
    "    return diarization_segments\n",
    "\n",
    "# -----------------------------\n",
    "# Transcription with Whisper\n",
    "# -----------------------------\n",
    "def perform_transcription(audio_file, model_name=WHISPER_MODEL):\n",
    "    model = whisper.load_model(model_name)\n",
    "    result = model.transcribe(audio_file)\n",
    "    # result['segments'] includes a list of segments with 'start', 'end', 'text'\n",
    "    return result['segments']\n",
    "\n",
    "# -----------------------------\n",
    "# Helper: Find Speaker for a Given Segment\n",
    "# We try to find which diarization segment overlaps most with the transcription segment.\n",
    "# -----------------------------\n",
    "def find_speaker_for_time(diarization_segments, seg_start, seg_end):\n",
    "    best_overlap = 0.0\n",
    "    best_speaker = \"Unknown\"\n",
    "    for (d_start, d_end, d_speaker) in diarization_segments:\n",
    "        # Calculate overlap\n",
    "        overlap = min(seg_end, d_end) - max(seg_start, d_start)\n",
    "        if overlap > best_overlap:\n",
    "            best_overlap = overlap\n",
    "            best_speaker = d_speaker\n",
    "    return best_speaker\n",
    "\n",
    "# -----------------------------\n",
    "# Main Integration\n",
    "# -----------------------------\n",
    "def main():\n",
    "    # Perform diarization\n",
    "    print(\"Performing diarization...\")\n",
    "    diarization_segments = perform_diarization(INPUT_AUDIO_FILE)\n",
    "    print(\"...Diarization complete.\")\n",
    "\n",
    "    # Perform transcription\n",
    "    print(\"Performing transcription...\")\n",
    "    transcription_segments = perform_transcription(INPUT_AUDIO_FILE, WHISPER_MODEL)\n",
    "    print(\"... Transcription complete.\")\n",
    "\n",
    "    # Merge results\n",
    "    print(\"Merging results...\")\n",
    "    merged_transcript = []\n",
    "    for seg in transcription_segments:\n",
    "        seg_start = seg[\"start\"]\n",
    "        seg_end = seg[\"end\"]\n",
    "        seg_text = seg[\"text\"].strip()\n",
    "\n",
    "        speaker_label = find_speaker_for_time(diarization_segments, seg_start, seg_end)\n",
    "\n",
    "        merged_transcript.append({\n",
    "            \"start\": seg_start,\n",
    "            \"end\": seg_end,\n",
    "            \"speaker\": speaker_label,\n",
    "            \"text\": seg_text\n",
    "        })\n",
    "    print(\"... Merging results complete.\")\n",
    "\n",
    "    # Print or save the results\n",
    "    # Here we print as JSON for demonstration:\n",
    "    print(json.dumps(merged_transcript, indent=2))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
